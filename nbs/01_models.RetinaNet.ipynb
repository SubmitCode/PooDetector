{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RetinaNet\n",
    "\n",
    "> An implementation of RetinaNet in fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This is an implementation of  [RetinaNet](https://arxiv.org/abs/1708.02002), which is based on a [Feature Pyramid Network](https://arxiv.org/abs/1612.03144). The basis of this notebook comes from [fastai's Pascal.ipynb](https://github.com/fastai/course-v3/blob/master/nbs/dl2/timing.ipynb). Usually we use the PascalVOC dataset for benchmarking. In this case we are going to use the horse poo data set as there is already a working version in fastai for the PascalVOC dataset.\n",
    "\n",
    "<img src=\"image/The-network-architecture-of-RetinaNet-RetinaNet-uses-the-Feature-Pyramid-Network-FPN.png\">\n",
    "Source: <a href=\"https://www.researchgate.net/figure/The-network-architecture-of-RetinaNet-RetinaNet-uses-the-Feature-Pyramid-Network-FPN_fig1_327737749\">https://www.researchgate.net/</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.RetinaNet\n",
    "# default_cls_lvl 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#from fastai.vision import *\n",
    "from PooDetector.dataset import *\n",
    "from fastai.vision import *\n",
    "from fastai.vision.models.unet import _get_sfs_idxs, model_sizes, hook_outputs\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as mcolors\n",
    "from cycler import cycler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess dataset\n",
    "#boxes = jsonl_convert(path='data/poo_detection_01.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's grab some test data\n",
    "#folder = 'data/poo_detection_01/' \n",
    "#boxes = get_labels_for_folder(folder)\n",
    "#transforms = [[flip_lr(p=0.5), brightness(change=(0.4, 0.6), p=0.75), contrast(scale=(0.8, 1.25), p=.75)],[]]\n",
    "\n",
    "#data = (ObjectItemList\n",
    "#        .from_folder(folder)\n",
    "#        .split_by_rand_pct(0.2)\n",
    "#        .label_from_func(lambda o: boxes[o.name])\n",
    "#        .transform(transforms, tfm_y=True, size=224)\n",
    "#        .databunch(bs=64, collate_fn=bb_pad_collate)\n",
    "       # .normalize(imagenet_stats)\n",
    "#       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LateralUpsampleMerge(nn.Module):\n",
    "    \"Merge the features coming from the downsample path (in `hook`) with the upsample path.\"\n",
    "    def __init__(self, ch, ch_lat, hook):\n",
    "        super().__init__()\n",
    "        self.hook = hook\n",
    "        self.conv_lat = conv2d(ch_lat, ch, ks=1, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_lat(self.hook.stored) + F.interpolate(x, self.hook.stored.shape[-2:], mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RetinaNet(nn.Module):\n",
    "    \"Implements RetinaNet from https://arxiv.org/abs/1708.02002\"\n",
    "    def __init__(self, encoder:nn.Module, n_classes, final_bias=0., chs=256, n_anchors=9, flatten=True):\n",
    "        super().__init__()\n",
    "        self.n_classes,self.flatten = n_classes,flatten\n",
    "        imsize = (256,256)\n",
    "        sfs_szs = model_sizes(encoder, size=imsize)\n",
    "        sfs_idxs = list(reversed(_get_sfs_idxs(sfs_szs)))\n",
    "        self.sfs = hook_outputs([encoder[i] for i in sfs_idxs])\n",
    "        self.encoder = encoder\n",
    "        self.c5top5 = conv2d(sfs_szs[-1][1], chs, ks=1, bias=True)\n",
    "        self.c5top6 = conv2d(sfs_szs[-1][1], chs, stride=2, bias=True)\n",
    "        self.p6top7 = nn.Sequential(nn.ReLU(), conv2d(chs, chs, stride=2, bias=True))\n",
    "        self.merges = nn.ModuleList([LateralUpsampleMerge(chs, sfs_szs[idx][1], hook) \n",
    "                                     for idx,hook in zip(sfs_idxs[-2:-4:-1], self.sfs[-2:-4:-1])])\n",
    "        self.smoothers = nn.ModuleList([conv2d(chs, chs, 3, bias=True) for _ in range(3)])\n",
    "        self.classifier = self._head_subnet(n_classes, n_anchors, final_bias, chs=chs)\n",
    "        self.box_regressor = self._head_subnet(4, n_anchors, 0., chs=chs)\n",
    "        \n",
    "    def _head_subnet(self, n_classes, n_anchors, final_bias=0., n_conv=4, chs=256):\n",
    "        \"Helper function to create one of the subnet for regression/classification.\"\n",
    "        layers = [conv_layer(chs, chs, bias=True, norm_type=None) for _ in range(n_conv)]\n",
    "        layers += [conv2d(chs, n_classes * n_anchors, bias=True)]\n",
    "        layers[-1].bias.data.zero_().add_(final_bias)\n",
    "        layers[-1].weight.data.fill_(0)\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _apply_transpose(self, func, p_states, n_classes):\n",
    "        #Final result of the classifier/regressor is bs * (k * n_anchors) * h * w\n",
    "        #We make it bs * h * w * n_anchors * k then flatten in bs * -1 * k so we can contenate\n",
    "        #all the results in bs * anchors * k (the non flatten version is there for debugging only)\n",
    "        if not self.flatten: \n",
    "            sizes = [[p.size(0), p.size(2), p.size(3)] for p in p_states]\n",
    "            return [func(p).permute(0,2,3,1).view(*sz,-1,n_classes) for p,sz in zip(p_states,sizes)]\n",
    "        else:\n",
    "            return torch.cat([func(p).permute(0,2,3,1).contiguous().view(p.size(0),-1,n_classes) for p in p_states],1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        c5 = self.encoder(x)\n",
    "        p_states = [self.c5top5(c5.clone()), self.c5top6(c5)]\n",
    "        p_states.append(self.p6top7(p_states[-1]))\n",
    "        for merge in self.merges: p_states = [merge(p_states[0])] + p_states\n",
    "        for i, smooth in enumerate(self.smoothers[:3]):\n",
    "            p_states[i] = smooth(p_states[i])\n",
    "        return [self._apply_transpose(self.classifier, p_states, self.n_classes), \n",
    "                self._apply_transpose(self.box_regressor, p_states, 4),\n",
    "                [[p.size(2), p.size(3)] for p in p_states]]\n",
    "    \n",
    "    def __del__(self):\n",
    "        if hasattr(self, \"sfs\"): self.sfs.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_grid(size):\n",
    "    \"Create a grid of a given `size`.\"\n",
    "    H, W = size if is_tuple(size) else (size,size)\n",
    "    grid = FloatTensor(H, W, 2)\n",
    "    linear_points = torch.linspace(-1+1/W, 1-1/W, W) if W > 1 else tensor([0.])\n",
    "    grid[:, :, 1] = torch.ger(torch.ones(H), linear_points).expand_as(grid[:, :, 0])\n",
    "    linear_points = torch.linspace(-1+1/H, 1-1/H, H) if H > 1 else tensor([0.])\n",
    "    grid[:, :, 0] = torch.ger(linear_points, torch.ones(W)).expand_as(grid[:, :, 1])\n",
    "    return grid.view(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_anchors(ancs, size):\n",
    "    _,ax = plt.subplots(1,1, figsize=(5,5))\n",
    "    ax.set_xticks(np.linspace(-1,1, size[1]+1))\n",
    "    ax.set_yticks(np.linspace(-1,1, size[0]+1))\n",
    "    ax.grid()\n",
    "    ax.scatter(ancs[:,1], ancs[:,0]) #y is first\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_xlim(-1,1)\n",
    "    ax.set_ylim(1,-1) #-1 is top, 1 is bottom\n",
    "    for i, (x, y) in enumerate(zip(ancs[:, 1], ancs[:, 0])): ax.annotate(i, xy = (x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_anchors(sizes, ratios, scales, flatten=True):\n",
    "    \"Create anchor of `sizes`, `ratios` and `scales`.\"\n",
    "    aspects = [[[s*math.sqrt(r), s*math.sqrt(1/r)] for s in scales] for r in ratios]\n",
    "    aspects = torch.tensor(aspects).view(-1,2)\n",
    "    anchors = []\n",
    "    for h,w in sizes:\n",
    "        #4 here to have the anchors overlap.\n",
    "        sized_aspects = 4 * (aspects * torch.tensor([2/h,2/w])).unsqueeze(0)\n",
    "        base_grid = create_grid((h,w)).unsqueeze(1)\n",
    "        n,a = base_grid.size(0),aspects.size(0)\n",
    "        ancs = torch.cat([base_grid.expand(n,a,2), sized_aspects.expand(n,a,2)], 2)\n",
    "        anchors.append(ancs.view(h,w,a,4))\n",
    "    return torch.cat([anc.view(-1,4) for anc in anchors],0) if flatten else anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = [1/2,1,2]\n",
    "scales = [1,2**(-1/3), 2**(-2/3)] \n",
    "#Paper used [1,2**(1/3), 2**(2/3)] but a bigger size (600) too, so the largest feature map gave anchors that cover less of the image.\n",
    "sizes = [(2**i,2**i) for i in range(5)]\n",
    "sizes.reverse() #Predictions come in the order of the smallest feature map to the biggest\n",
    "anchors = create_anchors(sizes, ratios, scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_cmap(N):\n",
    "    color_norm  = mcolors.Normalize(vmin=0, vmax=N-1)\n",
    "    return cmx.ScalarMappable(norm=color_norm, cmap='Set3').to_rgba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_color = 12\n",
    "cmap = get_cmap(num_color)\n",
    "color_list = [cmap(float(x)) for x in range(num_color)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def draw_outline(o, lw):\n",
    "    o.set_path_effects([patheffects.Stroke(\n",
    "        linewidth=lw, foreground='black'), patheffects.Normal()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def draw_rect(ax, b, color='white'):\n",
    "    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2))\n",
    "    draw_outline(patch, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def draw_text(ax, xy, txt, sz=14, color='white'):\n",
    "    text = ax.text(*xy, txt,\n",
    "        verticalalignment='top', color=color, fontsize=sz, weight='bold')\n",
    "    draw_outline(text, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_boxes(boxes):\n",
    "    \"Show the `boxes` (size by 4)\"\n",
    "    _, ax = plt.subplots(1,1, figsize=(5,5))\n",
    "    ax.set_xlim(-1,1)\n",
    "    ax.set_ylim(1,-1)\n",
    "    for i, bbox in enumerate(boxes):\n",
    "        bb = bbox.numpy()\n",
    "        rect = [bb[1]-bb[3]/2, bb[0]-bb[2]/2, bb[3], bb[2]]\n",
    "        draw_rect(ax, rect, color=color_list[i%num_color])\n",
    "        draw_text(ax, [bb[1]-bb[3]/2,bb[0]-bb[2]/2], str(i), color=color_list[i%num_color])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_boxes(anchors[900:909])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def activ_to_bbox(acts, anchors, flatten=True):\n",
    "    \"Extrapolate bounding boxes on anchors from the model activations.\"\n",
    "    if flatten:\n",
    "        acts.mul_(acts.new_tensor([[0.1, 0.1, 0.2, 0.2]])) #Can't remember where those scales come from, but they help regularize\n",
    "        centers = anchors[...,2:] * acts[...,:2] + anchors[...,:2]\n",
    "        sizes = anchors[...,2:] * torch.exp(acts[...,:2])\n",
    "        return torch.cat([centers, sizes], -1)\n",
    "    else: return [activ_to_bbox(act,anc) for act,anc in zip(acts, anchors)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cthw2tlbr(boxes):\n",
    "    \"Convert center/size format `boxes` to top/left bottom/right corners.\"\n",
    "    top_left = boxes[:,:2] - boxes[:,2:]/2\n",
    "    bot_right = boxes[:,:2] + boxes[:,2:]/2\n",
    "    return torch.cat([top_left, bot_right], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def intersection(anchors, targets):\n",
    "    \"Compute the sizes of the intersections of `anchors` by `targets`.\"\n",
    "    ancs, tgts = cthw2tlbr(anchors), cthw2tlbr(targets)\n",
    "    a, t = ancs.size(0), tgts.size(0)\n",
    "    ancs, tgts = ancs.unsqueeze(1).expand(a,t,4), tgts.unsqueeze(0).expand(a,t,4)\n",
    "    top_left_i = torch.max(ancs[...,:2], tgts[...,:2])\n",
    "    bot_right_i = torch.min(ancs[...,2:], tgts[...,2:])\n",
    "    sizes = torch.clamp(bot_right_i - top_left_i, min=0) \n",
    "    return sizes[...,0] * sizes[...,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def IoU_values(anchors, targets):\n",
    "    \"Compute the IoU values of `anchors` by `targets`.\"\n",
    "    inter = intersection(anchors, targets)\n",
    "    anc_sz, tgt_sz = anchors[:,2] * anchors[:,3], targets[:,2] * targets[:,3]\n",
    "    union = anc_sz.unsqueeze(1) + tgt_sz.unsqueeze(0) - inter\n",
    "    return inter/(union+1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def match_anchors(anchors, targets, match_thr=0.5, bkg_thr=0.4):\n",
    "    \"Match `anchors` to targets. -1 is match to background, -2 is ignore.\"\n",
    "    matches = anchors.new(anchors.size(0)).zero_().long() - 2\n",
    "    if targets.numel() == 0: return matches\n",
    "    ious = IoU_values(anchors, targets)\n",
    "    vals,idxs = torch.max(ious,1)\n",
    "    matches[vals < bkg_thr] = -1\n",
    "    matches[vals > match_thr] = idxs[vals > match_thr]\n",
    "    #Overwrite matches with each target getting the anchor that has the max IoU.\n",
    "    #vals,idxs = torch.max(ious,0)\n",
    "    #If idxs contains repetition, this doesn't bug and only the last is considered.\n",
    "    #matches[idxs] = targets.new_tensor(list(range(targets.size(0)))).long()\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tlbr2cthw(boxes):\n",
    "    \"Convert top/left bottom/right format `boxes` to center/size corners.\"\n",
    "    center = (boxes[:,:2] + boxes[:,2:])/2\n",
    "    sizes = boxes[:,2:] - boxes[:,:2]\n",
    "    return torch.cat([center, sizes], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def bbox_to_activ(bboxes, anchors, flatten=True):\n",
    "    \"Return the target of the model on `anchors` for the `bboxes`.\"\n",
    "    if flatten:\n",
    "        t_centers = (bboxes[...,:2] - anchors[...,:2]) / anchors[...,2:] \n",
    "        t_sizes = torch.log(bboxes[...,2:] / anchors[...,2:] + 1e-8) \n",
    "        return torch.cat([t_centers, t_sizes], -1).div_(bboxes.new_tensor([[0.1, 0.1, 0.2, 0.2]]))\n",
    "    else: return [activ_to_bbox(act,anc) for act,anc in zip(acts, anchors)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def encode_class(idxs, n_classes):\n",
    "    target = idxs.new_zeros(len(idxs), n_classes).float()\n",
    "    mask = idxs != 0\n",
    "    i1s = LongTensor(list(range(len(idxs))))\n",
    "    target[i1s[mask],idxs[mask]-1] = 1\n",
    "    return target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RetinaNetFocalLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, gamma:float=2., alpha:float=0.25,  pad_idx:int=0, scales:Collection[float]=None, \n",
    "                 ratios:Collection[float]=None, reg_loss:LossFunction=F.smooth_l1_loss):\n",
    "        super().__init__()\n",
    "        self.gamma,self.alpha,self.pad_idx,self.reg_loss = gamma,alpha,pad_idx,reg_loss\n",
    "        self.scales = ifnone(scales, [1,2**(-1/3), 2**(-2/3)])\n",
    "        self.ratios = ifnone(ratios, [1/2,1,2])\n",
    "        \n",
    "    def _change_anchors(self, sizes:Sizes) -> bool:\n",
    "        if not hasattr(self, 'sizes'): return True\n",
    "        for sz1, sz2 in zip(self.sizes, sizes):\n",
    "            if sz1[0] != sz2[0] or sz1[1] != sz2[1]: return True\n",
    "        return False\n",
    "    \n",
    "    def _create_anchors(self, sizes:Sizes, device:torch.device):\n",
    "        self.sizes = sizes\n",
    "        self.anchors = create_anchors(sizes, self.ratios, self.scales).to(device)\n",
    "    \n",
    "    def _unpad(self, bbox_tgt, clas_tgt):\n",
    "        i = torch.min(torch.nonzero(clas_tgt-self.pad_idx))\n",
    "        return tlbr2cthw(bbox_tgt[i:]), clas_tgt[i:]-1+self.pad_idx\n",
    "    \n",
    "    def _focal_loss(self, clas_pred, clas_tgt):\n",
    "        encoded_tgt = encode_class(clas_tgt, clas_pred.size(1))\n",
    "        ps = torch.sigmoid(clas_pred.detach())\n",
    "        weights = encoded_tgt * (1-ps) + (1-encoded_tgt) * ps\n",
    "        alphas = (1-encoded_tgt) * self.alpha + encoded_tgt * (1-self.alpha)\n",
    "        weights.pow_(self.gamma).mul_(alphas)\n",
    "        clas_loss = F.binary_cross_entropy_with_logits(clas_pred, encoded_tgt, weights, reduction='sum')\n",
    "        return clas_loss\n",
    "        \n",
    "    def _one_loss(self, clas_pred, bbox_pred, clas_tgt, bbox_tgt):\n",
    "        bbox_tgt, clas_tgt = self._unpad(bbox_tgt, clas_tgt)\n",
    "        matches = match_anchors(self.anchors, bbox_tgt)\n",
    "        bbox_mask = matches>=0\n",
    "        if bbox_mask.sum() != 0:\n",
    "            bbox_pred = bbox_pred[bbox_mask]\n",
    "            bbox_tgt = bbox_tgt[matches[bbox_mask]]\n",
    "            bb_loss = self.reg_loss(bbox_pred, bbox_to_activ(bbox_tgt, self.anchors[bbox_mask]))\n",
    "        else: bb_loss = 0.\n",
    "        matches.add_(1)\n",
    "        clas_tgt = clas_tgt + 1\n",
    "        clas_mask = matches>=0\n",
    "        clas_pred = clas_pred[clas_mask]\n",
    "        clas_tgt = torch.cat([clas_tgt.new_zeros(1).long(), clas_tgt])\n",
    "        clas_tgt = clas_tgt[matches[clas_mask]]\n",
    "        return bb_loss + self._focal_loss(clas_pred, clas_tgt)/torch.clamp(bbox_mask.sum(), min=1.)\n",
    "    \n",
    "    def forward(self, output, bbox_tgts, clas_tgts):\n",
    "        clas_preds, bbox_preds, sizes = output\n",
    "        if self._change_anchors(sizes): self._create_anchors(sizes, clas_preds.device)\n",
    "        n_classes = clas_preds.size(2)\n",
    "        return sum([self._one_loss(cp, bp, ct, bt)\n",
    "                    for (cp, bp, ct, bt) in zip(clas_preds, bbox_preds, clas_tgts, bbox_tgts)])/clas_tgts.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SigmaL1SmoothLoss(nn.Module):\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        reg_diff = torch.abs(target - output)\n",
    "        reg_loss = torch.where(torch.le(reg_diff, 1/9), 4.5 * torch.pow(reg_diff, 2), reg_diff - 1/18)\n",
    "        return reg_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = [1/2,1,2]\n",
    "#scales = [1,2**(-1/3), 2**(-2/3)]\n",
    "scales = [1,2**(1/3), 2**(2/3)] #for bigger size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder = create_body(models.resnet50, cut=-2)\n",
    "#model = RetinaNet(encoder, data.c, final_bias=-4)\n",
    "#crit = RetinaNetFocalLoss(scales=scales, ratios=ratios)\n",
    "#learn = Learner(data, model, loss_func=crit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def retina_net_split(model):\n",
    "    groups = [list(model.encoder.children())[:6], list(model.encoder.children())[6:]]\n",
    "    return groups + [list(model.children())[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn = learn.split(retina_net_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.fit_one_cycle(5, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.save('stage1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.fit_one_cycle(10, slice(1e-6, 5e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.save('stage2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.load('stage2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.fit_one_cycle(10, slice(1e-6, 5e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.save('stage3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.load('stage3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.fit_one_cycle(10, slice(1e-6, 5e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.save('stage4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn = learn.load('stage4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img,target = next(iter(data.valid_dl))\n",
    "#with torch.no_grad():\n",
    "#    output = learn.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def unpad(tgt_bbox, tgt_clas, pad_idx=0):\n",
    "    i = torch.min(torch.nonzero(tgt_clas-pad_idx))\n",
    "    return tlbr2cthw(tgt_bbox[i:]), tgt_clas[i:]-1+pad_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def process_output(output, i, detect_thresh=0.25):\n",
    "    \"Process `output[i]` and return the predicted bboxes above `detect_thresh`.\"\n",
    "    clas_pred,bbox_pred,sizes = output[0][i], output[1][i], output[2]\n",
    "    anchors = create_anchors(sizes, ratios, scales).to(clas_pred.device)\n",
    "    bbox_pred = activ_to_bbox(bbox_pred, anchors)\n",
    "    clas_pred = torch.sigmoid(clas_pred)\n",
    "    detect_mask = clas_pred.max(1)[0] > detect_thresh\n",
    "    bbox_pred, clas_pred = bbox_pred[detect_mask], clas_pred[detect_mask]\n",
    "    bbox_pred = tlbr2cthw(torch.clamp(cthw2tlbr(bbox_pred), min=-1, max=1))    \n",
    "    scores, preds = clas_pred.max(1)\n",
    "    return bbox_pred, scores, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _draw_outline(o:Patch, lw:int):\n",
    "    \"Outline bounding box onto image `Patch`.\"\n",
    "    o.set_path_effects([patheffects.Stroke(\n",
    "        linewidth=lw, foreground='black'), patheffects.Normal()])\n",
    "#export\n",
    "def draw_rect(ax:plt.Axes, b:Collection[int], color:str='white', text=None, text_size=14):\n",
    "    \"Draw bounding box on `ax`.\"\n",
    "    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2))\n",
    "    _draw_outline(patch, 4)\n",
    "    if text is not None:\n",
    "        patch = ax.text(*b[:2], text, verticalalignment='top', color=color, fontsize=text_size, weight='bold')\n",
    "        _draw_outline(patch,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_preds(img, output, idx, detect_thresh=0.25, classes=None):\n",
    "    bbox_pred, scores, preds = process_output(output, idx, detect_thresh)\n",
    "    bbox_pred, preds, scores = bbox_pred.cpu(), preds.cpu(), scores.cpu()\n",
    "    t_sz = torch.Tensor([*img.size])[None].float()\n",
    "    bbox_pred[:,:2] = bbox_pred[:,:2] - bbox_pred[:,2:]/2\n",
    "    bbox_pred[:,:2] = (bbox_pred[:,:2] + 1) * t_sz/2\n",
    "    bbox_pred[:,2:] = bbox_pred[:,2:] * t_sz\n",
    "    bbox_pred = bbox_pred.long()\n",
    "    _, ax = plt.subplots(1,1)\n",
    "    for bbox, c, scr in zip(bbox_pred, preds, scores):\n",
    "        img.show(ax=ax)\n",
    "        txt = str(c.item()) if classes is None else classes[c.item()+1]\n",
    "        draw_rect(ax, [bbox[1],bbox[0],bbox[3],bbox[2]], text=f'{txt} {scr:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx = 0\n",
    "#img = data.valid_ds[idx][0]\n",
    "#show_preds(img, output, idx, detect_thresh=0.3, classes=data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def nms(boxes, scores, thresh=0.3):\n",
    "    idx_sort = scores.argsort(descending=True)\n",
    "    boxes, scores = boxes[idx_sort], scores[idx_sort]\n",
    "    to_keep, indexes = [], torch.LongTensor(range_of(scores))\n",
    "    while len(scores) > 0:\n",
    "        to_keep.append(idx_sort[indexes[0]])\n",
    "        iou_vals = IoU_values(boxes, boxes[:1]).squeeze()\n",
    "        mask_keep = iou_vals < thresh\n",
    "        if len(mask_keep.nonzero()) == 0: break\n",
    "        boxes, scores, indexes = boxes[mask_keep], scores[mask_keep], indexes[mask_keep]\n",
    "    return LongTensor(to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def process_output(output, i, detect_thresh=0.25):\n",
    "    clas_pred,bbox_pred,sizes = output[0][i], output[1][i], output[2]\n",
    "    anchors = create_anchors(sizes, ratios, scales).to(clas_pred.device)\n",
    "    bbox_pred = activ_to_bbox(bbox_pred, anchors)\n",
    "    clas_pred = torch.sigmoid(clas_pred)\n",
    "    detect_mask = clas_pred.max(1)[0] > detect_thresh\n",
    "    bbox_pred, clas_pred = bbox_pred[detect_mask], clas_pred[detect_mask]\n",
    "    bbox_pred = tlbr2cthw(torch.clamp(cthw2tlbr(bbox_pred), min=-1, max=1))    \n",
    "    if clas_pred.numel() == 0: return [],[],[]\n",
    "    scores, preds = clas_pred.max(1)\n",
    "    return bbox_pred, scores, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_preds(img, output, idx, detect_thresh=0.25, classes=None, ax=None):\n",
    "    bbox_pred, scores, preds = process_output(output, idx, detect_thresh)\n",
    "    if len(scores) != 0:\n",
    "        to_keep = nms(bbox_pred, scores)\n",
    "        bbox_pred, preds, scores = bbox_pred[to_keep].cpu(), preds[to_keep].cpu(), scores[to_keep].cpu()\n",
    "        t_sz = torch.Tensor([*img.size])[None].float()\n",
    "        bbox_pred[:,:2] = bbox_pred[:,:2] - bbox_pred[:,2:]/2\n",
    "        bbox_pred[:,:2] = (bbox_pred[:,:2] + 1) * t_sz/2\n",
    "        bbox_pred[:,2:] = bbox_pred[:,2:] * t_sz\n",
    "        bbox_pred = bbox_pred.long()\n",
    "    if ax is None: _, ax = plt.subplots(1,1)\n",
    "    img.show(ax=ax)\n",
    "    for bbox, c, scr in zip(bbox_pred, preds, scores):\n",
    "        txt = str(c.item()) if classes is None else classes[c.item()+1]\n",
    "        draw_rect(ax, [bbox[1],bbox[0],bbox[3],bbox[2]], text=f'{txt} {scr:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_results(learn, start=0, n=5, detect_thresh=0.35, figsize=(10,25)):\n",
    "    x,y = learn.data.one_batch(DatasetType.Valid, cpu=False)\n",
    "    with torch.no_grad():\n",
    "        z = learn.model.eval()(x)\n",
    "    _,axs = plt.subplots(n, 2, figsize=figsize)\n",
    "    for i in range(n):\n",
    "        img,bbox = learn.data.valid_ds[start+i]\n",
    "        img.show(ax=axs[i,0], y=bbox)\n",
    "        show_preds(img, z, start+i, detect_thresh=detect_thresh, classes=learn.data.classes, ax=axs[i,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(learn.data.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_results(learn, start=25, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_predictions(output, idx, detect_thresh=0.05):\n",
    "    bbox_pred, scores, preds = process_output(output, idx, detect_thresh)\n",
    "    if len(scores) == 0: return [],[],[]\n",
    "    to_keep = nms(bbox_pred, scores)\n",
    "    return bbox_pred[to_keep], preds[to_keep], scores[to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compute_ap(precision, recall):\n",
    "    \"Compute the average precision for `precision` and `recall` curve.\"\n",
    "    recall = np.concatenate(([0.], list(recall), [1.]))\n",
    "    precision = np.concatenate(([0.], list(precision), [0.]))\n",
    "    for i in range(len(precision) - 1, 0, -1):\n",
    "        precision[i - 1] = np.maximum(precision[i - 1], precision[i])\n",
    "    idx = np.where(recall[1:] != recall[:-1])[0]\n",
    "    ap = np.sum((recall[idx + 1] - recall[idx]) * precision[idx + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compute_class_AP(model, dl, n_classes, iou_thresh=0.5, detect_thresh=0.35, num_keep=100):\n",
    "    tps, clas, p_scores = [], [], []\n",
    "    classes, n_gts = LongTensor(range(n_classes)),torch.zeros(n_classes).long()\n",
    "    with torch.no_grad():\n",
    "        for input,target in progress_bar(dl):\n",
    "            output = model(input)\n",
    "            for i in range(target[0].size(0)):\n",
    "                bbox_pred, preds, scores = get_predictions(output, i, detect_thresh)\n",
    "                tgt_bbox, tgt_clas = unpad(target[0][i], target[1][i])\n",
    "                if len(bbox_pred) != 0 and len(tgt_bbox) != 0:\n",
    "                    ious = IoU_values(bbox_pred, tgt_bbox)\n",
    "                    max_iou, matches = ious.max(1)\n",
    "                    detected = []\n",
    "                    for i in range_of(preds):\n",
    "                        if max_iou[i] >= iou_thresh and matches[i] not in detected and tgt_clas[matches[i]] == preds[i]:\n",
    "                            detected.append(matches[i])\n",
    "                            tps.append(1)\n",
    "                        else: tps.append(0)\n",
    "                    clas.append(preds.cpu())\n",
    "                    p_scores.append(scores.cpu())\n",
    "                n_gts += (tgt_clas.cpu()[:,None] == classes[None,:]).sum(0)\n",
    "    tps, p_scores, clas = torch.tensor(tps), torch.cat(p_scores,0), torch.cat(clas,0)\n",
    "    fps = 1-tps\n",
    "    idx = p_scores.argsort(descending=True)\n",
    "    tps, fps, clas = tps[idx], fps[idx], clas[idx]\n",
    "    aps = []\n",
    "    #return tps, clas\n",
    "    for cls in range(n_classes):\n",
    "        tps_cls, fps_cls = tps[clas==cls].float().cumsum(0), fps[clas==cls].float().cumsum(0)\n",
    "        if tps_cls.numel() != 0 and tps_cls[-1] != 0:\n",
    "            precision = tps_cls / (tps_cls + fps_cls + 1e-8)\n",
    "            recall = tps_cls / (n_gts[cls] + 1e-8)\n",
    "            aps.append(compute_ap(precision, recall))\n",
    "        else: aps.append(0.)\n",
    "    return aps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L = compute_class_AP(learn.model, data.valid_dl, data.c-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ap,cl in zip(L, data.classes[1:]): print(f'{cl}: {ap:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_dataset.ipynb.\n",
      "Converted 01_models.RetinaNet.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
