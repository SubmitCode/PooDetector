#AUTOGENERATED! DO NOT EDIT! File to edit: dev/00_core.ipynb (unless otherwise specified).

__all__ = ['ProdigyDataReader', 'LABELS', 'get_dataloader', 'collate_fn']

#Cell
import json
import nbdev
import re
import base64
import PIL
import pathlib
from io import BytesIO
from fastai.vision import *

#Cell
LABELS = ['FABI', 'FABI_POO', 'FABI_PISS', 'SOPHIE', 'HUMAN']


class ProdigyDataReader(object):
    """
    ProdigyDataReader
    """
    def __init__(self, root, transforms=None, object_categories=LABELS):
        self.root = pathlib.Path(root)
        self.transforms = transforms
        self.class_names = object_categories
        self.num_classes = len(object_categories)

        if self.root.exists() is False:
            raise AssertionError()

        self.images = [image for image in self.read_jsonl(str(self.root))]
        if len(self.images) == 0:
            raise AssertionError()

    @classmethod
    def read_jsonl(self, file_path):
        """Read a .jsonl file and yield its contents line by line.
        file_path (unicode / Path): The file path.
        YIELDS: The loaded JSON contents of each line.
        """
        with pathlib.Path(file_path).open('r', encoding='utf8') as f:
            for line in f:
                try:  # hack to handle broken jsonl
                    str_json = json.loads(line)
                    if str_json['answer'] == 'accept':
                        yield str_json
                except ValueError:
                    continue

    @classmethod
    def stringToRGB(self, base64_string):
        """ convert base64 string to cv2 image """
        base64_string = base64_string[23:]
        imgdata = base64.b64decode(str(base64_string))
        image = PIL.Image.open(io.BytesIO(imgdata))
        return np.array(image)


    @classmethod
    def write_jsonl(self, file_path, lines):
        """Create a .jsonl file and dump contents.
        file_path (unicode / Path): The path to the output file.
        lines (list): The JSON-serializable contents of each line.
        """
        data = [ujson.dumps(line, escape_forward_slashes=False) for line in lines]
        pathlib.Path(file_path).open('w', encoding='utf-8').write('\n'.join(data))

    def __getitem__(self, idx):
        """ get item of the index.
        In this particular case it is important to understand that the x-axis
        represents the width and the y-axis the height.
        """
        item = self.images[idx]

        img = self.stringToRGB(item['image'])

        boxes = []
        labels = []
        for element in item.get('spans', []):
            points = np.array(element['points']).round().astype(int)
            xmin = np.min(points[:, 0])
            ymin = np.min(points[:, 1])
            xmax = np.max(points[:, 0])
            ymax = np.max(points[:, 1])
            boxes.append([xmin, ymin, xmax, ymax])

            label = self.class_names.index(element['label'])
            labels.append(label)

        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)

        image_id = torch.as_tensor([idx])
        # suppose all instances are not crowd
        iscrowd = torch.zeros((len(boxes), ), dtype=torch.int64)

        target = {}
        target["boxes"] = boxes
        target["labels"] = labels
        target["image_id"] = image_id
        target["iscrowd"] = iscrowd

        #if self.transforms is not None:
        #    img, target = self.transforms(img, target)
        return img, target

    def __len__(self):
        return len(self.images)

#Cell
import torch

def get_dataloader(
        path='data/interim/data.json',
        train_test_split=0.8,
        batch_size_train=5,
        batch_size_test=5,
        shuffle_train=True,
        perm_images=True,
        transform_train=True,
        transform_test=False,
        class_names=None):
    """ get the dataloader objects """

    # use our dataset and defined transformations
    if class_names is None:
        dataset = ProdigyDataReader(path, get_transforms(train=transform_train))
        dataset_test = ProdigyDataReader(path, get_transforms(train=transform_test))
    else:
        dataset = ProdigyDataReader(path, get_transforms(train=transform_train),
                                    object_categories=class_names)
        dataset_test = ProdigyDataReader(path, get_transforms(train=transform_test),
                                         object_categories=class_names)

    # split the dataset in train and test set
    if perm_images:
        indices = torch.randperm(len(dataset)).tolist()
    else:
        indices = list(range(len(dataset)))

    len_train = int(len(indices) * train_test_split)
    dataset = torch.utils.data.Subset(dataset, indices[:len_train])
    dataset_test = torch.utils.data.Subset(dataset_test, indices[len_train:])

    # define training and validation data loaders
    data_loader = torch.utils.data.DataLoader(
        dataset, batch_size=batch_size_train, shuffle=shuffle_train, num_workers=0,
        collate_fn=collate_fn)

    data_loader_test = torch.utils.data.DataLoader(
        dataset_test, batch_size=batch_size_test, shuffle=False, num_workers=0,
        collate_fn=collate_fn)

    return [data_loader, data_loader_test]


def collate_fn(batch):
    return tuple(zip(*batch))